<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/FaceLandmarkerHelper.kt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/FaceLandmarkerHelper.kt" />
              <option name="originalContent" value="package com.amoherom.mizuface&#10;&#10;import android.content.Context&#10;import android.graphics.Bitmap&#10;import android.graphics.Matrix&#10;import android.media.MediaMetadataRetriever&#10;import android.net.Uri&#10;import android.os.SystemClock&#10;import android.util.Log&#10;import androidx.annotation.VisibleForTesting&#10;import androidx.camera.core.ImageProxy&#10;import com.google.mediapipe.framework.image.BitmapImageBuilder&#10;import com.google.mediapipe.framework.image.MPImage&#10;import com.google.mediapipe.tasks.core.BaseOptions&#10;import com.google.mediapipe.tasks.core.Delegate&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarker&#10;import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarkerResult&#10;&#10;class FaceLandmarkerHelper (&#10;    var minFaceDetectionConfidence: Float = DEFAULT_FACE_DETECTION_CONFIDENCE,&#10;    var minFaceTrackingConfidence: Float = DEFAULT_FACE_TRACKING_CONFIDENCE,&#10;    var minFacePresenceConfidence: Float = DEFAULT_FACE_PRESENCE_CONFIDENCE,&#10;    var maxNumFaces: Int = DEFAULT_NUM_FACES,&#10;    var currentDelegate: Int = DELEGATE_CPU,&#10;    var runningMode: RunningMode = RunningMode.IMAGE,&#10;    val context: Context,&#10;    // this listener is only used when running in RunningMode.LIVE_STREAM&#10;    val faceLandmarkerHelperListener: LandmarkerListener? = null&#10;    ) {&#10;&#10;        // For this example this needs to be a var so it can be reset on changes.&#10;        // If the Face Landmarker will not change, a lazy val would be preferable.&#10;        private var faceLandmarker: FaceLandmarker? = null&#10;&#10;        init {&#10;            setupFaceLandmarker()&#10;        }&#10;&#10;        fun clearFaceLandmarker() {&#10;            faceLandmarker?.close()&#10;            faceLandmarker = null&#10;        }&#10;&#10;        // Return running status of FaceLandmarkerHelper&#10;        fun isClose(): Boolean {&#10;            return faceLandmarker == null&#10;        }&#10;&#10;        // Initialize the Face landmarker using current settings on the&#10;        // thread that is using it. CPU can be used with Landmarker&#10;        // that are created on the main thread and used on a background thread, but&#10;        // the GPU delegate needs to be used on the thread that initialized the&#10;        // Landmarker&#10;        fun setupFaceLandmarker() {&#10;            // Set general face landmarker options&#10;            val baseOptionBuilder = BaseOptions.builder()&#10;&#10;            // Use the specified hardware for running the model. Default to CPU&#10;            when (currentDelegate) {&#10;                DELEGATE_CPU -&gt; {&#10;                    baseOptionBuilder.setDelegate(Delegate.CPU)&#10;                }&#10;                DELEGATE_GPU -&gt; {&#10;                    baseOptionBuilder.setDelegate(Delegate.GPU)&#10;                }&#10;            }&#10;&#10;            baseOptionBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)&#10;&#10;            // Check if runningMode is consistent with faceLandmarkerHelperListener&#10;            when (runningMode) {&#10;                RunningMode.LIVE_STREAM -&gt; {&#10;                    if (faceLandmarkerHelperListener == null) {&#10;                        throw IllegalStateException(&#10;                            &quot;faceLandmarkerHelperListener must be set when runningMode is LIVE_STREAM.&quot;&#10;                        )&#10;                    }&#10;                }&#10;                else -&gt; {&#10;                    // no-op&#10;                }&#10;            }&#10;&#10;            try {&#10;                val baseOptions = baseOptionBuilder.build()&#10;                // Create an option builder with base options and specific&#10;                // options only use for Face Landmarker.&#10;                val optionsBuilder =&#10;                    FaceLandmarker.FaceLandmarkerOptions.builder()&#10;                        .setBaseOptions(baseOptions)&#10;                        .setMinFaceDetectionConfidence(minFaceDetectionConfidence)&#10;                        .setMinTrackingConfidence(minFaceTrackingConfidence)&#10;                        .setMinFacePresenceConfidence(minFacePresenceConfidence)&#10;                        .setNumFaces(maxNumFaces)&#10;                        .setOutputFaceBlendshapes(true)&#10;                        .setRunningMode(runningMode)&#10;&#10;                // The ResultListener and ErrorListener only use for LIVE_STREAM mode.&#10;                if (runningMode == RunningMode.LIVE_STREAM) {&#10;                    optionsBuilder&#10;                        .setResultListener(this::returnLivestreamResult)&#10;                        .setErrorListener(this::returnLivestreamError)&#10;                }&#10;&#10;                val options = optionsBuilder.build()&#10;                faceLandmarker =&#10;                    FaceLandmarker.createFromOptions(context, options)&#10;            } catch (e: IllegalStateException) {&#10;                faceLandmarkerHelperListener?.onError(&#10;                    &quot;Face Landmarker failed to initialize. See error logs for &quot; +&#10;                            &quot;details&quot;&#10;                )&#10;                Log.e(&#10;                    TAG, &quot;MediaPipe failed to load the task with error: &quot; + e&#10;                        .message&#10;                )&#10;            } catch (e: RuntimeException) {&#10;                // This occurs if the model being used does not support GPU&#10;                faceLandmarkerHelperListener?.onError(&#10;                    &quot;Face Landmarker failed to initialize. See error logs for &quot; +&#10;                            &quot;details&quot;, GPU_ERROR&#10;                )&#10;                Log.e(&#10;                    TAG,&#10;                    &quot;Face Landmarker failed to load model with error: &quot; + e.message&#10;                )&#10;            }&#10;        }&#10;&#10;        // Convert the ImageProxy to MP Image and feed it to FacelandmakerHelper.&#10;        fun detectLiveStream(&#10;            imageProxy: ImageProxy,&#10;            isFrontCamera: Boolean&#10;        ) {&#10;            if (runningMode != RunningMode.LIVE_STREAM) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectLiveStream&quot; +&#10;                            &quot; while not using RunningMode.LIVE_STREAM&quot;&#10;                )&#10;            }&#10;            val frameTime = SystemClock.uptimeMillis()&#10;&#10;            // Copy out RGB bits from the frame to a bitmap buffer&#10;            val bitmapBuffer =&#10;                Bitmap.createBitmap(&#10;                    imageProxy.width,&#10;                    imageProxy.height,&#10;                    Bitmap.Config.ARGB_8888&#10;                )&#10;            imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }&#10;            imageProxy.close()&#10;&#10;            val matrix = Matrix().apply {&#10;                // Rotate the frame received from the camera to be in the same direction as it'll be shown&#10;                postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())&#10;&#10;                // flip image if user use front camera&#10;                if (isFrontCamera) {&#10;                    postScale(&#10;                        -1f,&#10;                        1f,&#10;                        imageProxy.width.toFloat(),&#10;                        imageProxy.height.toFloat()&#10;                    )&#10;                }&#10;            }&#10;            val rotatedBitmap = Bitmap.createBitmap(&#10;                bitmapBuffer, 0, 0, bitmapBuffer.width, bitmapBuffer.height,&#10;                matrix, true&#10;            )&#10;&#10;            // Convert the input Bitmap object to an MPImage object to run inference&#10;            val mpImage = BitmapImageBuilder(rotatedBitmap).build()&#10;&#10;            detectAsync(mpImage, frameTime)&#10;        }&#10;&#10;        // Run face face landmark using MediaPipe Face Landmarker API&#10;        @VisibleForTesting&#10;        fun detectAsync(mpImage: MPImage, frameTime: Long) {&#10;            faceLandmarker?.detectAsync(mpImage, frameTime)&#10;            // As we're using running mode LIVE_STREAM, the landmark result will&#10;            // be returned in returnLivestreamResult function&#10;        }&#10;&#10;        // Accepts the URI for a video file loaded from the user's gallery and attempts to run&#10;        // face landmarker inference on the video. This process will evaluate every&#10;        // frame in the video and attach the results to a bundle that will be&#10;        // returned.&#10;        fun detectVideoFile(&#10;            videoUri: Uri,&#10;            inferenceIntervalMs: Long&#10;        ): VideoResultBundle? {&#10;            if (runningMode != RunningMode.VIDEO) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectVideoFile&quot; +&#10;                            &quot; while not using RunningMode.VIDEO&quot;&#10;                )&#10;            }&#10;&#10;            // Inference time is the difference between the system time at the start and finish of the&#10;            // process&#10;            val startTime = SystemClock.uptimeMillis()&#10;&#10;            var didErrorOccurred = false&#10;&#10;            // Load frames from the video and run the face landmarker.&#10;            val retriever = MediaMetadataRetriever()&#10;            retriever.setDataSource(context, videoUri)&#10;            val videoLengthMs =&#10;                retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)&#10;                    ?.toLong()&#10;&#10;            // Note: We need to read width/height from frame instead of getting the width/height&#10;            // of the video directly because MediaRetriever returns frames that are smaller than the&#10;            // actual dimension of the video file.&#10;            val firstFrame = retriever.getFrameAtTime(0)&#10;            val width = firstFrame?.width&#10;            val height = firstFrame?.height&#10;&#10;            // If the video is invalid, returns a null detection result&#10;            if ((videoLengthMs == null) || (width == null) || (height == null)) return null&#10;&#10;            // Next, we'll get one frame every frameInterval ms, then run detection on these frames.&#10;            val resultList = mutableListOf&lt;FaceLandmarkerResult&gt;()&#10;            val numberOfFrameToRead = videoLengthMs.div(inferenceIntervalMs)&#10;&#10;            for (i in 0..numberOfFrameToRead) {&#10;                val timestampMs = i * inferenceIntervalMs // ms&#10;&#10;                retriever&#10;                    .getFrameAtTime(&#10;                        timestampMs * 1000, // convert from ms to micro-s&#10;                        MediaMetadataRetriever.OPTION_CLOSEST&#10;                    )&#10;                    ?.let { frame -&gt;&#10;                        // Convert the video frame to ARGB_8888 which is required by the MediaPipe&#10;                        val argb8888Frame =&#10;                            if (frame.config == Bitmap.Config.ARGB_8888) frame&#10;                            else frame.copy(Bitmap.Config.ARGB_8888, false)&#10;&#10;                        // Convert the input Bitmap object to an MPImage object to run inference&#10;                        val mpImage = BitmapImageBuilder(argb8888Frame).build()&#10;&#10;                        // Run face landmarker using MediaPipe Face Landmarker API&#10;                        faceLandmarker?.detectForVideo(mpImage, timestampMs)&#10;                            ?.let { detectionResult -&gt;&#10;                                resultList.add(detectionResult)&#10;                            } ?: {&#10;                            didErrorOccurred = true&#10;                            faceLandmarkerHelperListener?.onError(&#10;                                &quot;ResultBundle could not be returned&quot; +&#10;                                        &quot; in detectVideoFile&quot;&#10;                            )&#10;                        }&#10;                    }&#10;                    ?: run {&#10;                        didErrorOccurred = true&#10;                        faceLandmarkerHelperListener?.onError(&#10;                            &quot;Frame at specified time could not be&quot; +&#10;                                    &quot; retrieved when detecting in video.&quot;&#10;                        )&#10;                    }&#10;            }&#10;&#10;            retriever.release()&#10;&#10;            val inferenceTimePerFrameMs =&#10;                (SystemClock.uptimeMillis() - startTime).div(numberOfFrameToRead)&#10;&#10;            return if (didErrorOccurred) {&#10;                null&#10;            } else {&#10;                VideoResultBundle(resultList, inferenceTimePerFrameMs, height, width)&#10;            }&#10;        }&#10;&#10;        // Accepted a Bitmap and runs face landmarker inference on it to return&#10;        // results back to the caller&#10;        fun detectImage(image: Bitmap): ResultBundle? {&#10;            if (runningMode != RunningMode.IMAGE) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectImage&quot; +&#10;                            &quot; while not using RunningMode.IMAGE&quot;&#10;                )&#10;            }&#10;&#10;&#10;            // Inference time is the difference between the system time at the&#10;            // start and finish of the process&#10;            val startTime = SystemClock.uptimeMillis()&#10;&#10;            // Convert the input Bitmap object to an MPImage object to run inference&#10;            val mpImage = BitmapImageBuilder(image).build()&#10;&#10;            // Run face landmarker using MediaPipe Face Landmarker API&#10;            faceLandmarker?.detect(mpImage)?.also { landmarkResult -&gt;&#10;                val inferenceTimeMs = SystemClock.uptimeMillis() - startTime&#10;                return ResultBundle(&#10;                    landmarkResult,&#10;                    inferenceTimeMs,&#10;                    image.height,&#10;                    image.width&#10;                )&#10;            }&#10;&#10;            // If faceLandmarker?.detect() returns null, this is likely an error. Returning null&#10;            // to indicate this.&#10;            faceLandmarkerHelperListener?.onError(&#10;                &quot;Face Landmarker failed to detect.&quot;&#10;            )&#10;            return null&#10;        }&#10;&#10;        // Return the landmark result to this FaceLandmarkerHelper's caller&#10;        private fun returnLivestreamResult(&#10;            result: FaceLandmarkerResult,&#10;            input: MPImage&#10;        ) {&#10;            if( result.faceLandmarks().size &gt; 0 ) {&#10;                val finishTimeMs = SystemClock.uptimeMillis()&#10;                val inferenceTime = finishTimeMs - result.timestampMs()&#10;&#10;                faceLandmarkerHelperListener?.onResults(&#10;                    ResultBundle(&#10;                        result,&#10;                        inferenceTime,&#10;                        input.height,&#10;                        input.width&#10;                    )&#10;                )&#10;            }&#10;            else {&#10;                faceLandmarkerHelperListener?.onEmpty()&#10;            }&#10;        }&#10;&#10;        // Return errors thrown during detection to this FaceLandmarkerHelper's&#10;        // caller&#10;        private fun returnLivestreamError(error: RuntimeException) {&#10;            faceLandmarkerHelperListener?.onError(&#10;                error.message ?: &quot;An unknown error has occurred&quot;&#10;            )&#10;        }&#10;&#10;        companion object {&#10;            const val TAG = &quot;FaceLandmarkerHelper&quot;&#10;            private const val MP_FACE_LANDMARKER_TASK = &quot;face_landmarker.task&quot;&#10;&#10;            const val DELEGATE_CPU = 0&#10;            const val DELEGATE_GPU = 1&#10;            const val DEFAULT_FACE_DETECTION_CONFIDENCE = 0.5F&#10;            const val DEFAULT_FACE_TRACKING_CONFIDENCE = 0.5F&#10;            const val DEFAULT_FACE_PRESENCE_CONFIDENCE = 0.5F&#10;            const val DEFAULT_NUM_FACES = 1&#10;            const val OTHER_ERROR = 0&#10;            const val GPU_ERROR = 1&#10;        }&#10;&#10;        data class ResultBundle(&#10;            val result: FaceLandmarkerResult,&#10;            val inferenceTime: Long,&#10;            val inputImageHeight: Int,&#10;            val inputImageWidth: Int,&#10;        )&#10;&#10;        data class VideoResultBundle(&#10;            val results: List&lt;FaceLandmarkerResult&gt;,&#10;            val inferenceTime: Long,&#10;            val inputImageHeight: Int,&#10;            val inputImageWidth: Int,&#10;        )&#10;&#10;        interface LandmarkerListener {&#10;            fun onError(error: String, errorCode: Int = OTHER_ERROR)&#10;            fun onResults(resultBundle: ResultBundle)&#10;&#10;            fun onEmpty() {}&#10;        }&#10;    }&#10;" />
              <option name="updatedContent" value="package com.amoherom.mizuface&#10;&#10;import android.content.Context&#10;import android.graphics.Bitmap&#10;import android.graphics.Matrix&#10;import android.media.MediaMetadataRetriever&#10;import android.net.Uri&#10;import android.os.SystemClock&#10;import android.util.Log&#10;import androidx.annotation.VisibleForTesting&#10;import androidx.camera.core.ImageProxy&#10;import com.google.mediapipe.framework.image.BitmapImageBuilder&#10;import com.google.mediapipe.framework.image.MPImage&#10;import com.google.mediapipe.tasks.core.BaseOptions&#10;import com.google.mediapipe.tasks.core.Delegate&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarker&#10;import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarkerResult&#10;&#10;class FaceLandmarkerHelper (&#10;    var minFaceDetectionConfidence: Float = DEFAULT_FACE_DETECTION_CONFIDENCE,&#10;    var minFaceTrackingConfidence: Float = DEFAULT_FACE_TRACKING_CONFIDENCE,&#10;    var minFacePresenceConfidence: Float = DEFAULT_FACE_PRESENCE_CONFIDENCE,&#10;    var maxNumFaces: Int = DEFAULT_NUM_FACES,&#10;    var currentDelegate: Int = DELEGATE_CPU,&#10;    var runningMode: RunningMode = RunningMode.IMAGE,&#10;    val context: Context,&#10;    // this listener is only used when running in RunningMode.LIVE_STREAM&#10;    val faceLandmarkerHelperListener: LandmarkerListener? = null&#10;    ) {&#10;&#10;        // For this example this needs to be a var so it can be reset on changes.&#10;        // If the Face Landmarker will not change, a lazy val would be preferable.&#10;        private var faceLandmarker: FaceLandmarker? = null&#10;&#10;        init {&#10;            setupFaceLandmarker()&#10;        }&#10;&#10;        fun clearFaceLandmarker() {&#10;            faceLandmarker?.close()&#10;            faceLandmarker = null&#10;        }&#10;&#10;        // Return running status of FaceLandmarkerHelper&#10;        fun isClose(): Boolean {&#10;            return faceLandmarker == null&#10;        }&#10;&#10;        // Initialize the Face landmarker using current settings on the&#10;        // thread that is using it. CPU can be used with Landmarker&#10;        // that are created on the main thread and used on a background thread, but&#10;        // the GPU delegate needs to be used on the thread that initialized the&#10;        // Landmarker&#10;        fun setupFaceLandmarker() {&#10;            // Set general face landmarker options&#10;            val baseOptionBuilder = BaseOptions.builder()&#10;&#10;            // Use the specified hardware for running the model. Default to CPU&#10;            when (currentDelegate) {&#10;                DELEGATE_CPU -&gt; {&#10;                    baseOptionBuilder.setDelegate(Delegate.CPU)&#10;                }&#10;                DELEGATE_GPU -&gt; {&#10;                    baseOptionBuilder.setDelegate(Delegate.GPU)&#10;                }&#10;            }&#10;&#10;            baseOptionBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)&#10;&#10;            // Check if runningMode is consistent with faceLandmarkerHelperListener&#10;            when (runningMode) {&#10;                RunningMode.LIVE_STREAM -&gt; {&#10;                    if (faceLandmarkerHelperListener == null) {&#10;                        throw IllegalStateException(&#10;                            &quot;faceLandmarkerHelperListener must be set when runningMode is LIVE_STREAM.&quot;&#10;                        )&#10;                    }&#10;                }&#10;                else -&gt; {&#10;                    // no-op&#10;                }&#10;            }&#10;&#10;            try {&#10;                val baseOptions = baseOptionBuilder.build()&#10;                // Create an option builder with base options and specific&#10;                // options only use for Face Landmarker.&#10;                val optionsBuilder =&#10;                    FaceLandmarker.FaceLandmarkerOptions.builder()&#10;                        .setBaseOptions(baseOptions)&#10;                        .setMinFaceDetectionConfidence(minFaceDetectionConfidence)&#10;                        .setMinTrackingConfidence(minFaceTrackingConfidence)&#10;                        .setMinFacePresenceConfidence(minFacePresenceConfidence)&#10;                        .setNumFaces(maxNumFaces)&#10;                        .setOutputFaceBlendshapes(true)&#10;                        .setRunningMode(runningMode)&#10;&#10;                // The ResultListener and ErrorListener only use for LIVE_STREAM mode.&#10;                if (runningMode == RunningMode.LIVE_STREAM) {&#10;                    optionsBuilder&#10;                        .setResultListener(this::returnLivestreamResult)&#10;                        .setErrorListener(this::returnLivestreamError)&#10;                }&#10;&#10;                val options = optionsBuilder.build()&#10;                faceLandmarker =&#10;                    FaceLandmarker.createFromOptions(context, options)&#10;            } catch (e: IllegalStateException) {&#10;                faceLandmarkerHelperListener?.onError(&#10;                    &quot;Face Landmarker failed to initialize. See error logs for &quot; +&#10;                            &quot;details&quot;&#10;                )&#10;                Log.e(&#10;                    TAG, &quot;MediaPipe failed to load the task with error: &quot; + e&#10;                        .message&#10;                )&#10;            } catch (e: RuntimeException) {&#10;                // This occurs if the model being used does not support GPU&#10;                faceLandmarkerHelperListener?.onError(&#10;                    &quot;Face Landmarker failed to initialize. See error logs for &quot; +&#10;                            &quot;details&quot;, GPU_ERROR&#10;                )&#10;                Log.e(&#10;                    TAG,&#10;                    &quot;Face Landmarker failed to load model with error: &quot; + e.message&#10;                )&#10;            }&#10;        }&#10;&#10;        // Convert the ImageProxy to MP Image and feed it to FacelandmakerHelper.&#10;        fun detectLiveStream(&#10;            imageProxy: ImageProxy,&#10;            isFrontCamera: Boolean&#10;        ) {&#10;            if (runningMode != RunningMode.LIVE_STREAM) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectLiveStream&quot; +&#10;                            &quot; while not using RunningMode.LIVE_STREAM&quot;&#10;                )&#10;            }&#10;            val frameTime = SystemClock.uptimeMillis()&#10;&#10;            // Copy out RGB bits from the frame to a bitmap buffer&#10;            val bitmapBuffer =&#10;                Bitmap.createBitmap(&#10;                    imageProxy.width,&#10;                    imageProxy.height,&#10;                    Bitmap.Config.ARGB_8888&#10;                )&#10;            imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }&#10;            imageProxy.close()&#10;&#10;            val matrix = Matrix().apply {&#10;                // Rotate the frame received from the camera to be in the same direction as it'll be shown&#10;                postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())&#10;&#10;                // flip image if user use front camera&#10;                if (isFrontCamera) {&#10;                    postScale(&#10;                        -1f,&#10;                        1f,&#10;                        imageProxy.width.toFloat(),&#10;                        imageProxy.height.toFloat()&#10;                    )&#10;                }&#10;            }&#10;            val rotatedBitmap = Bitmap.createBitmap(&#10;                bitmapBuffer, 0, 0, bitmapBuffer.width, bitmapBuffer.height,&#10;                matrix, true&#10;            )&#10;&#10;            // Convert the input Bitmap object to an MPImage object to run inference&#10;            val mpImage = BitmapImageBuilder(rotatedBitmap).build()&#10;&#10;            detectAsync(mpImage, frameTime)&#10;        }&#10;&#10;        // Run face face landmark using MediaPipe Face Landmarker API&#10;        @VisibleForTesting&#10;        fun detectAsync(mpImage: MPImage, frameTime: Long) {&#10;            faceLandmarker?.detectAsync(mpImage, frameTime)&#10;            // As we're using running mode LIVE_STREAM, the landmark result will&#10;            // be returned in returnLivestreamResult function&#10;        }&#10;&#10;        // Accepts the URI for a video file loaded from the user's gallery and attempts to run&#10;        // face landmarker inference on the video. This process will evaluate every&#10;        // frame in the video and attach the results to a bundle that will be&#10;        // returned.&#10;        fun detectVideoFile(&#10;            videoUri: Uri,&#10;            inferenceIntervalMs: Long&#10;        ): VideoResultBundle? {&#10;            if (runningMode != RunningMode.VIDEO) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectVideoFile&quot; +&#10;                            &quot; while not using RunningMode.VIDEO&quot;&#10;                )&#10;            }&#10;&#10;            // Inference time is the difference between the system time at the start and finish of the&#10;            // process&#10;            val startTime = SystemClock.uptimeMillis()&#10;&#10;            var didErrorOccurred = false&#10;&#10;            // Load frames from the video and run the face landmarker.&#10;            val retriever = MediaMetadataRetriever()&#10;            retriever.setDataSource(context, videoUri)&#10;            val videoLengthMs =&#10;                retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)&#10;                    ?.toLong()&#10;&#10;            // Note: We need to read width/height from frame instead of getting the width/height&#10;            // of the video directly because MediaRetriever returns frames that are smaller than the&#10;            // actual dimension of the video file.&#10;            val firstFrame = retriever.getFrameAtTime(0)&#10;            val width = firstFrame?.width&#10;            val height = firstFrame?.height&#10;&#10;            // If the video is invalid, returns a null detection result&#10;            if ((videoLengthMs == null) || (width == null) || (height == null)) return null&#10;&#10;            // Next, we'll get one frame every frameInterval ms, then run detection on these frames.&#10;            val resultList = mutableListOf&lt;FaceLandmarkerResult&gt;()&#10;            val numberOfFrameToRead = videoLengthMs.div(inferenceIntervalMs)&#10;&#10;            for (i in 0..numberOfFrameToRead) {&#10;                val timestampMs = i * inferenceIntervalMs // ms&#10;&#10;                retriever&#10;                    .getFrameAtTime(&#10;                        timestampMs * 1000, // convert from ms to micro-s&#10;                        MediaMetadataRetriever.OPTION_CLOSEST&#10;                    )&#10;                    ?.let { frame -&gt;&#10;                        // Convert the video frame to ARGB_8888 which is required by the MediaPipe&#10;                        val argb8888Frame =&#10;                            if (frame.config == Bitmap.Config.ARGB_8888) frame&#10;                            else frame.copy(Bitmap.Config.ARGB_8888, false)&#10;&#10;                        // Convert the input Bitmap object to an MPImage object to run inference&#10;                        val mpImage = BitmapImageBuilder(argb8888Frame).build()&#10;&#10;                        // Run face landmarker using MediaPipe Face Landmarker API&#10;                        faceLandmarker?.detectForVideo(mpImage, timestampMs)&#10;                            ?.let { detectionResult -&gt;&#10;                                resultList.add(detectionResult)&#10;                            } ?: {&#10;                            didErrorOccurred = true&#10;                            faceLandmarkerHelperListener?.onError(&#10;                                &quot;ResultBundle could not be returned&quot; +&#10;                                        &quot; in detectVideoFile&quot;&#10;                            )&#10;                        }&#10;                    }&#10;                    ?: run {&#10;                        didErrorOccurred = true&#10;                        faceLandmarkerHelperListener?.onError(&#10;                            &quot;Frame at specified time could not be&quot; +&#10;                                    &quot; retrieved when detecting in video.&quot;&#10;                        )&#10;                    }&#10;            }&#10;&#10;            retriever.release()&#10;&#10;            val inferenceTimePerFrameMs =&#10;                (SystemClock.uptimeMillis() - startTime).div(numberOfFrameToRead)&#10;&#10;            return if (didErrorOccurred) {&#10;                null&#10;            } else {&#10;                VideoResultBundle(resultList, inferenceTimePerFrameMs, height, width)&#10;            }&#10;        }&#10;&#10;        // Accepted a Bitmap and runs face landmarker inference on it to return&#10;        // results back to the caller&#10;        fun detectImage(image: Bitmap): ResultBundle? {&#10;            if (runningMode != RunningMode.IMAGE) {&#10;                throw IllegalArgumentException(&#10;                    &quot;Attempting to call detectImage&quot; +&#10;                            &quot; while not using RunningMode.IMAGE&quot;&#10;                )&#10;            }&#10;&#10;&#10;            // Inference time is the difference between the system time at the&#10;            // start and finish of the process&#10;            val startTime = SystemClock.uptimeMillis()&#10;&#10;            // Convert the input Bitmap object to an MPImage object to run inference&#10;            val mpImage = BitmapImageBuilder(image).build()&#10;&#10;            // Run face landmarker using MediaPipe Face Landmarker API&#10;            faceLandmarker?.detect(mpImage)?.also { landmarkResult -&gt;&#10;                val inferenceTimeMs = SystemClock.uptimeMillis() - startTime&#10;                return ResultBundle(&#10;                    landmarkResult,&#10;                    inferenceTimeMs,&#10;                    image.height,&#10;                    image.width&#10;                )&#10;            }&#10;&#10;            // If faceLandmarker?.detect() returns null, this is likely an error. Returning null&#10;            // to indicate this.&#10;            faceLandmarkerHelperListener?.onError(&#10;                &quot;Face Landmarker failed to detect.&quot;&#10;            )&#10;            return null&#10;        }&#10;&#10;        // Return the landmark result to this FaceLandmarkerHelper's caller&#10;        private fun returnLivestreamResult(&#10;            result: FaceLandmarkerResult,&#10;            input: MPImage&#10;        ) {&#10;            if( result.faceLandmarks().size &gt; 0 ) {&#10;                val finishTimeMs = SystemClock.uptimeMillis()&#10;                val inferenceTime = finishTimeMs - result.timestampMs()&#10;&#10;                faceLandmarkerHelperListener?.onResults(&#10;                    ResultBundle(&#10;                        result,&#10;                        inferenceTime,&#10;                        input.height,&#10;                        input.width&#10;                    )&#10;                )&#10;            }&#10;            else {&#10;                faceLandmarkerHelperListener?.onEmpty()&#10;            }&#10;        }&#10;&#10;        // Return errors thrown during detection to this FaceLandmarkerHelper's&#10;        // caller&#10;        private fun returnLivestreamError(error: RuntimeException) {&#10;            faceLandmarkerHelperListener?.onError(&#10;                error.message ?: &quot;An unknown error has occurred&quot;&#10;            )&#10;        }&#10;&#10;        companion object {&#10;            const val TAG = &quot;FaceLandmarkerHelper&quot;&#10;            private const val MP_FACE_LANDMARKER_TASK = &quot;face_landmarker.task&quot;&#10;&#10;            const val DELEGATE_CPU = 0&#10;            const val DELEGATE_GPU = 1&#10;            const val DEFAULT_FACE_DETECTION_CONFIDENCE = 0.5F&#10;            const val DEFAULT_FACE_TRACKING_CONFIDENCE = 0.5F&#10;            const val DEFAULT_FACE_PRESENCE_CONFIDENCE = 0.5F&#10;            const val DEFAULT_NUM_FACES = 1&#10;            const val OTHER_ERROR = 0&#10;            const val GPU_ERROR = 1&#10;        }&#10;&#10;        data class ResultBundle(&#10;            val result: FaceLandmarkerResult,&#10;            val inferenceTime: Long,&#10;            val inputImageHeight: Int,&#10;            val inputImageWidth: Int,&#10;        )&#10;&#10;        data class VideoResultBundle(&#10;            val results: List&lt;FaceLandmarkerResult&gt;,&#10;            val inferenceTime: Long,&#10;            val inputImageHeight: Int,&#10;            val inputImageWidth: Int,&#10;        )&#10;&#10;        interface LandmarkerListener {&#10;            fun onError(error: String, errorCode: Int = OTHER_ERROR)&#10;            fun onResults(resultBundle: ResultBundle)&#10;&#10;            fun onEmpty() {}&#10;        }&#10;    }" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/MainViewModel.kt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/MainViewModel.kt" />
              <option name="originalContent" value="package com.amoherom.mizuface&#10;&#10;import androidx.lifecycle.ViewModel&#10;&#10;/**&#10; *  This ViewModel is used to store face landmarker helper settings&#10; */&#10;class MainViewModel : ViewModel() {&#10;&#10;    private var _delegate: Int = FaceLandmarkerHelper.DELEGATE_CPU&#10;    private var _minFaceDetectionConfidence: Float =&#10;        FaceLandmarkerHelper.DEFAULT_FACE_DETECTION_CONFIDENCE&#10;    private var _minFaceTrackingConfidence: Float = FaceLandmarkerHelper&#10;        .DEFAULT_FACE_TRACKING_CONFIDENCE&#10;    private var _minFacePresenceConfidence: Float = FaceLandmarkerHelper&#10;        .DEFAULT_FACE_PRESENCE_CONFIDENCE&#10;    private var _maxFaces: Int = FaceLandmarkerHelper.DEFAULT_NUM_FACES&#10;&#10;    val currentDelegate: Int get() = _delegate&#10;    val currentMinFaceDetectionConfidence: Float&#10;        get() =&#10;            _minFaceDetectionConfidence&#10;    val currentMinFaceTrackingConfidence: Float&#10;        get() =&#10;            _minFaceTrackingConfidence&#10;    val currentMinFacePresenceConfidence: Float&#10;        get() =&#10;            _minFacePresenceConfidence&#10;    val currentMaxFaces: Int get() = _maxFaces&#10;&#10;    fun setDelegate(delegate: Int) {&#10;        _delegate = delegate&#10;    }&#10;&#10;    fun setMinFaceDetectionConfidence(confidence: Float) {&#10;        _minFaceDetectionConfidence = confidence&#10;    }&#10;    fun setMinFaceTrackingConfidence(confidence: Float) {&#10;        _minFaceTrackingConfidence = confidence&#10;    }&#10;    fun setMinFacePresenceConfidence(confidence: Float) {&#10;        _minFacePresenceConfidence = confidence&#10;    }&#10;&#10;    fun setMaxFaces(maxResults: Int) {&#10;        _maxFaces = maxResults&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.amoherom.mizuface&#13;&#10;&#13;&#10;import androidx.lifecycle.ViewModel&#13;&#10;&#13;&#10;/**&#13;&#10; *  This ViewModel is used to store face landmarker helper settings&#13;&#10; */&#13;&#10;class MainViewModel : ViewModel() {&#13;&#10;&#13;&#10;    private var _delegate: Int = FaceLandmarkerHelper.DELEGATE_GPU&#13;&#10;    private var _minFaceDetectionConfidence: Float =&#13;&#10;        FaceLandmarkerHelper.DEFAULT_FACE_DETECTION_CONFIDENCE&#13;&#10;    private var _minFaceTrackingConfidence: Float = FaceLandmarkerHelper&#13;&#10;        .DEFAULT_FACE_TRACKING_CONFIDENCE&#13;&#10;    private var _minFacePresenceConfidence: Float = FaceLandmarkerHelper&#13;&#10;        .DEFAULT_FACE_PRESENCE_CONFIDENCE&#13;&#10;    private var _maxFaces: Int = FaceLandmarkerHelper.DEFAULT_NUM_FACES&#13;&#10;&#13;&#10;    val currentDelegate: Int get() = _delegate&#13;&#10;    val currentMinFaceDetectionConfidence: Float&#13;&#10;        get() =&#13;&#10;            _minFaceDetectionConfidence&#13;&#10;    val currentMinFaceTrackingConfidence: Float&#13;&#10;        get() =&#13;&#10;            _minFaceTrackingConfidence&#13;&#10;    val currentMinFacePresenceConfidence: Float&#13;&#10;        get() =&#13;&#10;            _minFacePresenceConfidence&#13;&#10;    val currentMaxFaces: Int get() = _maxFaces&#13;&#10;&#13;&#10;    fun setDelegate(delegate: Int) {&#13;&#10;        _delegate = delegate&#13;&#10;    }&#13;&#10;&#13;&#10;    fun setMinFaceDetectionConfidence(confidence: Float) {&#13;&#10;        _minFaceDetectionConfidence = confidence&#13;&#10;    }&#13;&#10;    fun setMinFaceTrackingConfidence(confidence: Float) {&#13;&#10;        _minFaceTrackingConfidence = confidence&#13;&#10;    }&#13;&#10;    fun setMinFacePresenceConfidence(confidence: Float) {&#13;&#10;        _minFacePresenceConfidence = confidence&#13;&#10;    }&#13;&#10;&#13;&#10;    fun setMaxFaces(maxResults: Int) {&#13;&#10;        _maxFaces = maxResults&#13;&#10;    }&#13;&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/fragment/PermissionsFragment.kt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/fragment/PermissionsFragment.kt" />
              <option name="originalContent" value="package com.amoherom.mizuface.fragment&#10;&#10;import android.Manifest&#10;import android.content.Context&#10;import android.content.pm.PackageManager&#10;import android.os.Bundle&#10;import android.widget.Toast&#10;import androidx.activity.result.contract.ActivityResultContracts&#10;import androidx.core.content.ContextCompat&#10;import androidx.fragment.app.Fragment&#10;import androidx.lifecycle.lifecycleScope&#10;import androidx.navigation.Navigation&#10;import com.amoherom.mizuface.R&#10;&#10;private val PERMISSIONS_REQUIRED = arrayOf(Manifest.permission.CAMERA)&#10;&#10;class PermissionsFragment : Fragment() {&#10;&#10;    private val requestPermissionLauncher =&#10;        registerForActivityResult(&#10;            ActivityResultContracts.RequestPermission()&#10;        ) { isGranted: Boolean -&gt;&#10;            if (isGranted) {&#10;                Toast.makeText(&#10;                    context,&#10;                    &quot;Permission request granted&quot;,&#10;                    Toast.LENGTH_LONG&#10;                ).show()&#10;                navigateToCamera()&#10;            } else {&#10;                Toast.makeText(&#10;                    context,&#10;                    &quot;Permission request denied&quot;,&#10;                    Toast.LENGTH_LONG&#10;                ).show()&#10;            }&#10;        }&#10;&#10;    override fun onCreate(savedInstanceState: Bundle?) {&#10;        super.onCreate(savedInstanceState)&#10;        when (PackageManager.PERMISSION_GRANTED) {&#10;            ContextCompat.checkSelfPermission(&#10;                requireContext(),&#10;                Manifest.permission.CAMERA&#10;            ) -&gt; {&#10;                navigateToCamera()&#10;            }&#10;            else -&gt; {&#10;                requestPermissionLauncher.launch(&#10;                    Manifest.permission.CAMERA&#10;                )&#10;            }&#10;        }&#10;    }&#10;&#10;    private fun navigateToCamera() {&#10;        lifecycleScope.launchWhenStarted {&#10;            Navigation.findNavController(&#10;                requireActivity(),&#10;                R.id.fragment_container&#10;            ).navigate(&#10;                R.id.action_permissions_fragment_to_vtuberPCFragment&#10;            )&#10;        }&#10;    }&#10;&#10;    companion object {&#10;&#10;        /** Convenience method used to check if all permissions required by this app are granted */&#10;        fun hasPermissions(context: Context) = PERMISSIONS_REQUIRED.all {&#10;            ContextCompat.checkSelfPermission(&#10;                context,&#10;                it&#10;            ) == PackageManager.PERMISSION_GRANTED&#10;        }&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package com.amoherom.mizuface.fragment&#10;&#10;import android.Manifest&#10;import android.content.Context&#10;import android.content.pm.PackageManager&#10;import android.os.Bundle&#10;import android.view.LayoutInflater&#10;import android.view.View&#10;import android.view.ViewGroup&#10;import android.widget.Button&#10;import android.widget.TextView&#10;import android.widget.Toast&#10;import androidx.activity.result.contract.ActivityResultContracts&#10;import androidx.core.content.ContextCompat&#10;import androidx.fragment.app.Fragment&#10;import androidx.lifecycle.lifecycleScope&#10;import androidx.navigation.Navigation&#10;import com.amoherom.mizuface.R&#10;&#10;private val PERMISSIONS_REQUIRED = arrayOf(Manifest.permission.CAMERA)&#10;&#10;class PermissionsFragment : Fragment() {&#10;&#10;    private var rootView: View? = null&#10;&#10;    private val requestPermissionLauncher =&#10;        registerForActivityResult(&#10;            ActivityResultContracts.RequestPermission()&#10;        ) { isGranted: Boolean -&gt;&#10;            if (isGranted) {&#10;                Toast.makeText(&#10;                    context,&#10;                    &quot;Permission request granted&quot;,&#10;                    Toast.LENGTH_LONG&#10;                ).show()&#10;                navigateToCamera()&#10;            } else {&#10;                Toast.makeText(&#10;                    context,&#10;                    &quot;Permission request denied&quot;,&#10;                    Toast.LENGTH_LONG&#10;                ).show()&#10;                showPermissionDeniedUI()&#10;            }&#10;        }&#10;&#10;    override fun onCreateView(&#10;        inflater: LayoutInflater,&#10;        container: ViewGroup?,&#10;        savedInstanceState: Bundle?&#10;    ): View? {&#10;        rootView = inflater.inflate(R.layout.fragment_permissions, container, false)&#10;        return rootView&#10;    }&#10;&#10;    override fun onCreate(savedInstanceState: Bundle?) {&#10;        super.onCreate(savedInstanceState)&#10;&#10;        checkCameraPermission()&#10;    }&#10;&#10;    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {&#10;        super.onViewCreated(view, savedInstanceState)&#10;&#10;        view.findViewById&lt;Button&gt;(R.id.button_request_permission)?.setOnClickListener {&#10;            requestPermissionLauncher.launch(Manifest.permission.CAMERA)&#10;        }&#10;    }&#10;&#10;    private fun checkCameraPermission() {&#10;        when (PackageManager.PERMISSION_GRANTED) {&#10;            ContextCompat.checkSelfPermission(&#10;                requireContext(),&#10;                Manifest.permission.CAMERA&#10;            ) -&gt; {&#10;                navigateToCamera()&#10;            }&#10;            else -&gt; {&#10;                if (rootView == null) {&#10;                    // If view isn't created yet, the permission request will be handled in onCreate&#10;                    requestPermissionLauncher.launch(&#10;                        Manifest.permission.CAMERA&#10;                    )&#10;                } else {&#10;                    showPermissionDeniedUI()&#10;                }&#10;            }&#10;        }&#10;    }&#10;&#10;    private fun showPermissionDeniedUI() {&#10;        rootView?.let { view -&gt;&#10;            view.findViewById&lt;TextView&gt;(R.id.text_permission_explanation)?.visibility = View.VISIBLE&#10;            view.findViewById&lt;Button&gt;(R.id.button_request_permission)?.visibility = View.VISIBLE&#10;        }&#10;    }&#10;&#10;    private fun navigateToCamera() {&#10;        lifecycleScope.launchWhenStarted {&#10;            Navigation.findNavController(&#10;                requireActivity(),&#10;                R.id.fragment_container&#10;            ).navigate(&#10;                R.id.action_permissions_fragment_to_vtuberPCFragment&#10;            )&#10;        }&#10;    }&#10;&#10;    companion object {&#10;&#10;        /** Convenience method used to check if all permissions required by this app are granted */&#10;        fun hasPermissions(context: Context) = PERMISSIONS_REQUIRED.all {&#10;            ContextCompat.checkSelfPermission(&#10;                context,&#10;                it&#10;            ) == PackageManager.PERMISSION_GRANTED&#10;        }&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/fragment/VtuberPCFragment.kt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/java/com/amoherom/mizuface/fragment/VtuberPCFragment.kt" />
              <option name="originalContent" value="package com.amoherom.mizuface.fragment&#10;&#10;import android.annotation.SuppressLint&#10;import android.os.Bundle&#10;import android.util.Log&#10;import android.view.LayoutInflater&#10;import android.view.View&#10;import android.view.ViewGroup&#10;import android.widget.Toast&#10;import androidx.camera.core.AspectRatio&#10;import androidx.camera.core.Camera&#10;import androidx.camera.core.CameraSelector&#10;import androidx.camera.core.ImageAnalysis&#10;import androidx.camera.core.ImageProxy&#10;import androidx.camera.core.Preview&#10;import androidx.camera.lifecycle.ProcessCameraProvider&#10;import androidx.core.content.ContextCompat&#10;import androidx.fragment.app.Fragment&#10;import androidx.fragment.app.activityViewModels&#10;import androidx.navigation.Navigation&#10;import androidx.recyclerview.widget.LinearLayoutManager&#10;import androidx.recyclerview.widget.RecyclerView&#10;import com.amoherom.mizuface.FaceLandmarkerHelper&#10;import com.amoherom.mizuface.MainViewModel&#10;import com.amoherom.mizuface.R&#10;import com.amoherom.mizuface.databinding.FragmentVtuberPcBinding&#10;import com.amoherom.mizuface.fragment.FaceBlendshapesResultAdapter&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import kotlinx.coroutines.CoroutineScope&#10;import kotlinx.coroutines.Dispatchers&#10;import kotlinx.coroutines.launch&#10;import java.net.DatagramPacket&#10;import java.net.DatagramSocket&#10;import java.net.InetAddress&#10;import java.nio.charset.Charset&#10;import java.util.concurrent.ExecutorService&#10;import java.util.concurrent.Executors&#10;import androidx.navigation.findNavController&#10;&#10;class VtuberPCFragment : Fragment(), FaceLandmarkerHelper.LandmarkerListener {&#10;&#10;    // This fragment is used to send blendshapes to VSeeFace&#10;    // It can be used to display the blendshapes in a UI or send them over a network&#10;&#10;    private var PC_IP = &quot;192.168.1.2&quot;&#10;    private var PC_PORT = &quot;50509&quot; // VSeeFace default port Vnyan 50509&#10;&#10;&#10;    companion object {&#10;        private const val TAG = &quot;Face Landmarker&quot;&#10;    }&#10;&#10;    private var _binding: FragmentVtuberPcBinding? = null&#10;    private val binding get() = _binding!!&#10;&#10;    private val faceBlendshapesResultAdapter by lazy {&#10;        FaceBlendshapesResultAdapter()&#10;    }&#10;&#10;    private lateinit var faceLandmarkerHelper: FaceLandmarkerHelper&#10;&#10;    private val viewModel: MainViewModel by activityViewModels()&#10;&#10;    private var preview: Preview? = null&#10;    private var imageAnalyzer: ImageAnalysis? = null&#10;    private var camera: Camera? = null&#10;    private var cameraProvider: ProcessCameraProvider? = null&#10;    private var cameraFacing = CameraSelector.LENS_FACING_FRONT&#10;&#10;    private lateinit var backgroundExecutor: ExecutorService&#10;&#10;    private var pcSocket: DatagramSocket? = null&#10;&#10;    override fun onResume() {&#10;        super.onResume()&#10;        // Make sure that all permissions are still present, since the&#10;        // user could have removed them while the app was in paused state.&#10;        if (!PermissionsFragment.hasPermissions(requireContext())) {&#10;            Navigation.findNavController(&#10;                requireActivity(), R.id.fragment_container&#10;            ).navigate(R.id.action_vtuberPCFragment_to_permissions_fragment)&#10;        }&#10;&#10;        // Start the FaceLandmarkerHelper again when users come back&#10;        // to the foreground.&#10;        backgroundExecutor.execute {&#10;            if (faceLandmarkerHelper.isClose()) {&#10;                faceLandmarkerHelper.setupFaceLandmarker()&#10;            }&#10;        }&#10;    }&#10;&#10;    override fun onCreateView(&#10;        inflater: LayoutInflater,&#10;        container: ViewGroup?,&#10;        savedInstanceState: Bundle?&#10;    ): View {&#10;        _binding = FragmentVtuberPcBinding.inflate(inflater, container, false)&#10;        return binding.root&#10;    }&#10;&#10;    override fun onDestroyView() {&#10;        super.onDestroyView()&#10;        _binding = null&#10;        ClosePCConnection()&#10;    }&#10;&#10;    private fun setUpCamera(){&#10;        val cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())&#10;        cameraProviderFuture.addListener(&#10;            {&#10;                cameraProvider = cameraProviderFuture.get()&#10;                bindCameraUseCases()&#10;            }, ContextCompat.getMainExecutor(requireContext())&#10;        )&#10;    }&#10;&#10;    @SuppressLint(&quot;MissingPermission&quot;)&#10;    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {&#10;        super.onViewCreated(view, savedInstanceState)&#10;&#10;        with(binding.recyclerViewResults){&#10;            layoutManager = LinearLayoutManager(requireContext())&#10;            adapter = faceBlendshapesResultAdapter&#10;        }&#10;&#10;        backgroundExecutor = Executors.newSingleThreadExecutor()&#10;&#10;        // Check permissions before setting up camera&#10;        if (!PermissionsFragment.hasPermissions(requireContext())) {&#10;            requireActivity().findNavController(R.id.fragment_container).navigate(R.id.action_vtuberPCFragment_to_permissions_fragment)&#10;            return&#10;        }&#10;&#10;        binding.viewFinder.post{&#10;            setUpCamera()&#10;        }&#10;&#10;        backgroundExecutor.execute {&#10;            faceLandmarkerHelper = FaceLandmarkerHelper(&#10;                context = requireContext(),&#10;                runningMode = RunningMode.LIVE_STREAM,&#10;                minFaceDetectionConfidence = viewModel.currentMinFaceDetectionConfidence,&#10;                minFaceTrackingConfidence = viewModel.currentMinFaceTrackingConfidence,&#10;                minFacePresenceConfidence = viewModel.currentMinFacePresenceConfidence,&#10;                maxNumFaces = viewModel.currentMaxFaces,&#10;                currentDelegate = viewModel.currentDelegate,&#10;                faceLandmarkerHelperListener = this&#10;            )&#10;        }&#10;&#10;        // Initialize the PC connection&#10;        InitiatePCConnection()&#10;    }&#10;&#10;    private fun InitiatePCConnection() {&#10;        // This function can be used to initiate a connection to the PC&#10;        // For example, creating a socket connection or HTTP request&#10;        Log.d(TAG, &quot;Initiating connection to PC at $PC_IP:$PC_PORT&quot;)&#10;        try {&#10;            pcSocket = DatagramSocket()&#10;            binding.pcLinkState.setImageResource(R.drawable.link)&#10;            Log.d(TAG, &quot;PC Connection initiated successfully&quot;)&#10;        } catch (e: Exception) {&#10;            binding.pcLinkState.setImageResource(R.drawable.errorlink)&#10;            Log.e(TAG, &quot;Error initiating PC connection&quot;, e)&#10;        }&#10;    }&#10;&#10;    private fun ClosePCConnection(){&#10;        // This function can be used to close the connection to the PC&#10;        // For example, closing a socket connection or HTTP request&#10;        Log.d(TAG, &quot;Closing connection to PC at $PC_IP:$PC_PORT&quot;)&#10;        // Implement the logic to close the connection here&#10;        try {&#10;            pcSocket?.close()&#10;            Log.d(TAG, &quot;PC Connection closed successfully&quot;)&#10;        } catch (e: Exception) {&#10;            Log.e(TAG, &quot;Error closing PC connection&quot;, e)&#10;        }&#10;    }&#10;    private fun sendBlendshapesToPC(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        Rotation: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        Position: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeRight: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;    ) {&#10;        // This function can be used to send blendshapes to a PC&#10;        // For example, using a socket connection or HTTP request&#10;        // Here we just log the blendshapes&#10;        CoroutineScope(Dispatchers.IO).launch {&#10;            val json = buildJson(blendshapes, Position, Rotation, eyeLeft, eyeRight)&#10;            val buffer = json.toByteArray(Charset.forName(&quot;UTF-8&quot;))&#10;            val packet = DatagramPacket(&#10;                buffer,&#10;                buffer.size,&#10;                InetAddress.getByName(PC_IP),&#10;                PC_PORT.toInt()&#10;            )&#10;&#10;&#10;            if (pcSocket == null) {&#10;                binding.pcLinkState.setImageResource(R.drawable.errorlink)&#10;                Log.e(TAG, &quot;PC Socket is not initialized. Cannot send blendshapes.&quot;)&#10;            }&#10;&#10;            pcSocket?.send(packet)&#10;            //Log.d(TAG, &quot;Blendshapes sent to PC: $json&quot;)&#10;        }&#10;&#10;    }&#10;&#10;    private fun buildJson(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        position: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        rotation: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeRight: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f)&#10;    ): String {&#10;&#10;        val finalJson = buildTrackerFaceJson(&#10;            blendshapes = blendshapes,&#10;            position = position, // Placeholder for position&#10;            rotation = rotation, // Placeholder for rotation&#10;            vnYanPos = Triple(0.0f, 0.0f, 0.0f), // Placeholder for VNyan position&#10;            eyeLeft = eyeLeft, // Placeholder for left eye position&#10;            eyeRight = eyeRight // Placeholder for right eye position&#10;        )&#10;&#10;        return finalJson&#10;    }&#10;&#10;    private fun buildTrackerFaceJson(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        rotation: Triple&lt;Float, Float, Float&gt;,&#10;        position: Triple&lt;Float, Float, Float&gt;,&#10;        vnYanPos: Triple&lt;Float, Float, Float&gt;,&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt;,&#10;        eyeRight: Triple&lt;Float, Float, Float&gt;&#10;    ): String {&#10;        val blendshapesJson = blendshapes.entries.joinToString(&quot;,&quot;) {&#10;            &quot;&quot;&quot;{&quot;k&quot;:&quot;${it.key}&quot;,&quot;v&quot;:${it.value}}&quot;&quot;&quot;&#10;        }&#10;&#10;        return &quot;&quot;&quot;&#10;    {&#10;        &quot;Timestamp&quot;: ${System.currentTimeMillis()},&#10;        &quot;Hotkey&quot;: -1,&#10;        &quot;FaceFound&quot;: true,&#10;        &quot;Rotation&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, rotation.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, rotation.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, rotation.third)} },&#10;        &quot;Position&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, position.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, position.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, position.third)} },&#10;        &quot;EyeLeft&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.third)} },&#10;        &quot;EyeRight&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.third)} },&#10;        &quot;BlendShapes&quot;: [ $blendshapesJson ]&#10;    }&#10;    &quot;&quot;&quot;.trimIndent().replace(Regex(&quot;\\s+&quot;), &quot;&quot;)&#10;    }&#10;&#10;&#10;    @SuppressLint(&quot;UnsafeOptInUsageError&quot;)&#10;    private fun bindCameraUseCases() {&#10;&#10;        // CameraProvider&#10;        val cameraProvider = cameraProvider&#10;            ?: throw IllegalStateException(&quot;Camera initialization failed.&quot;)&#10;&#10;        val cameraSelector =&#10;            CameraSelector.Builder().requireLensFacing(cameraFacing).build()&#10;&#10;        // Preview. Only using the 4:3 ratio because this is the closest to our models&#10;        preview = Preview.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)&#10;            .setTargetRotation(binding.viewFinder.display.rotation)&#10;            .build()&#10;&#10;        // ImageAnalysis. Using RGBA 8888 to match how our models work&#10;        imageAnalyzer =&#10;            ImageAnalysis.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)&#10;                .setTargetRotation(binding.viewFinder.display.rotation)&#10;                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)&#10;                .setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)&#10;                .build()&#10;                // The analyzer can then be assigned to the instance&#10;                .also {&#10;                    it.setAnalyzer(backgroundExecutor) { image -&gt;&#10;                        detectFace(image)&#10;                    }&#10;                }&#10;&#10;        // Must unbind the use-cases before rebinding them&#10;        cameraProvider.unbindAll()&#10;&#10;        try {&#10;            // A variable number of use-cases can be passed here -&#10;            // camera provides access to CameraControl &amp; CameraInfo&#10;            camera = cameraProvider.bindToLifecycle(&#10;                this, cameraSelector, preview, imageAnalyzer&#10;            )&#10;&#10;            // Attach the viewfinder's surface provider to preview use case&#10;            preview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)&#10;        } catch (exc: Exception) {&#10;            Log.e(TAG, &quot;Use case binding failed&quot;, exc)&#10;        }&#10;    }&#10;&#10;    private fun detectFace(imageProxy: ImageProxy) {&#10;        faceLandmarkerHelper.detectLiveStream(&#10;            imageProxy = imageProxy,&#10;            isFrontCamera = cameraFacing == CameraSelector.LENS_FACING_FRONT&#10;        )&#10;    }&#10;&#10;    override fun onError(error: String, errorCode: Int) {&#10;        activity?.runOnUiThread {&#10;            Toast.makeText(requireContext(), error, Toast.LENGTH_SHORT).show()&#10;            faceBlendshapesResultAdapter.updateResults(null)&#10;            faceBlendshapesResultAdapter.notifyDataSetChanged()&#10;&#10;            if (errorCode == FaceLandmarkerHelper.GPU_ERROR) {&#10;                Log.d(&quot;VtuberPCFragment&quot;, &quot;GPU_ERROR&quot; + errorCode.toString())&#10;            }&#10;        }&#10;    }&#10;&#10;    fun toAvatarEyeCoords(x: Float, y: Float): Triple&lt;Float, Float, Float&gt; {&#10;        val scaledX = (x - 0.5f) * 40f // center and scale&#10;        val scaledY = (0.5f - y) * 50f // flip Y and scale&#10;        return Triple(scaledX, scaledY, 0f)&#10;    }&#10;&#10;    fun calcEyeOffsetXY(iris: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        left: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        right: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        top: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        bottom: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?&#10;    ): Pair&lt;Float, Float&gt; {&#10;        val horizontal = (((iris?.x() ?: 0f) - left?.x()!!) / ((right?.x() ?: 0f) - left.x()) - 0.5f) * 2f // -1 to 1&#10;        val vertical = (((iris?.y() ?: 0f ) - top?.y()!!) / ((bottom?.y() ?: 0f) - top.y()) - 0.5f) * 2f&#10;        return Pair(horizontal, vertical)&#10;    }&#10;&#10;&#10;&#10;    override fun onResults(resultBundle: FaceLandmarkerHelper.ResultBundle) {&#10;        activity?.runOnUiThread {&#10;            if (_binding != null){&#10;                if(binding.recyclerViewResults.scrollState != RecyclerView.SCROLL_STATE_DRAGGING) {&#10;                    faceBlendshapesResultAdapter.updateResults(resultBundle.result)&#10;                    faceBlendshapesResultAdapter.notifyDataSetChanged()&#10;                }&#10;&#10;                var blendhsapes = resultBundle.result?.faceBlendshapes()&#10;                var faceLandmarks = resultBundle.result?.faceLandmarks()&#10;                var noseLandmarkindex = faceLandmarks?.get(0)?.get(1)&#10;                var mouthLandmarkIndex = faceLandmarks?.get(0)?.get(0)&#10;&#10;                var leftEyeLandmarkIndex = faceLandmarks?.get(0)?.get(33)&#10;                var leftEyeLEFT = faceLandmarks?.get(0)?.get(133)&#10;                var leftEyeRIGHT = faceLandmarks?.get(0)?.get(33)&#10;                var leftEyeTOP = faceLandmarks?.get(0)?.get(159)&#10;                var leftEyeBOTTOM = faceLandmarks?.get(0)?.get(145)&#10;&#10;                var leftIrisLandmarkIndex = faceLandmarks?.get(0)?.get(468)&#10;                var leftIrisx = leftIrisLandmarkIndex?.x()?.toFloat() ?: 0f&#10;                var leftIrisy = leftIrisLandmarkIndex?.y()?.toFloat() ?: 0f&#10;&#10;                var rightEyeLandmarkIndex = faceLandmarks?.get(0)?.get(263)&#10;                var rightEyeLEFT = faceLandmarks?.get(0)?.get(362)&#10;                var rightEyeRIGHT = faceLandmarks?.get(0)?.get(263)&#10;                var rightEyeTOP = faceLandmarks?.get(0)?.get(386)&#10;                var rightEyeBOTTOM = faceLandmarks?.get(0)?.get(374)&#10;&#10;                var rightIrisLandmarkIndex = faceLandmarks?.get(0)?.get(473)&#10;                var rightIrisx = rightIrisLandmarkIndex?.x()?.toFloat() ?: 0f&#10;                var rightIrisy = rightIrisLandmarkIndex?.y()?.toFloat() ?: 0f&#10;&#10;                val (eyeLX, eyeLY) = calcEyeOffsetXY(&#10;                    iris = leftIrisLandmarkIndex,&#10;                    left = leftEyeLEFT,&#10;                    right = leftEyeRIGHT,&#10;                    top = leftEyeTOP,&#10;                    bottom = leftEyeBOTTOM&#10;                )&#10;&#10;                val (eyeRX, eyeRY) = calcEyeOffsetXY(&#10;                    iris = rightIrisLandmarkIndex,&#10;                    left = rightEyeLEFT,&#10;                    right = rightEyeRIGHT,&#10;                    top = rightEyeTOP,&#10;                    bottom = rightEyeBOTTOM&#10;                )&#10;&#10;                //Log.d(&quot;EyeTrack&quot;, &quot;LeftEye X: $eyeLX, Y: $eyeLY&quot;)&#10;&#10;                // upto 468 landmarks is the base&#10;                // with iris its up to 478 landmarks&#10;&#10;                // Yaw (side turn) → difference in eye X positions&#10;                val dxYaw = (rightEyeLandmarkIndex?.x() ?: 0f) - (leftEyeLandmarkIndex?.x() ?: 0f)&#10;                val dyYaw = (rightEyeLandmarkIndex?.z() ?: 0f) - (leftEyeLandmarkIndex?.z() ?: 0f)&#10;                val yaw = Math.atan2(dxYaw.toDouble(), dyYaw.toDouble()).toFloat()&#10;&#10;// Pitch (up/down) → difference between nose and eyes&#10;                val eyeY = ((leftEyeLandmarkIndex?.y() ?: 0f) + (rightEyeLandmarkIndex?.y() ?: 0f)) / 2&#10;                val pitch = Math.atan2((noseLandmarkindex?.y() ?: 0f) - eyeY.toDouble(), 1.0).toFloat()&#10;&#10;// Roll stays same (tilt head side)&#10;                val roll = Math.atan2(&#10;                    ((rightEyeLandmarkIndex?.y() ?: 0f) - (leftEyeLandmarkIndex?.y() ?: 0f)).toDouble(),&#10;                    ((rightEyeLandmarkIndex?.x() ?: 0f) - (leftEyeLandmarkIndex?.x() ?: 0f)).toDouble()&#10;                ).toFloat()&#10;&#10;&#10;&#10;//                var faceRotation = Triple((pitch * 100 / Math.PI).toFloat(), (yaw ).toFloat(), (roll * 100  / Math.PI).toFloat())&#10;                var faceRotation = Triple(&#10;                    50f - (yaw.toFloat() * 90 / Math.PI.toFloat()),&#10;                    pitch.toFloat() * 1000f / Math.PI.toFloat(),&#10;                    -roll.toFloat() * 100f / Math.PI.toFloat(),&#10;                )&#10;                var facePosition = Triple(0.0f, 0.0f, 0.0f)&#10;                var eyeLeft = Triple(0.5f, 0.5f, 0.5f)&#10;                var eyeRight = Triple(0.5f, 0.5f, 0.5f)&#10;&#10;                if (faceLandmarks != null &amp;&amp; faceLandmarks.isNotEmpty()) {&#10;                    facePosition = Triple(&#10;                        0f,&#10;                        0f,&#10;                        0f&#10;                    )&#10;&#10;                    eyeLeft = Triple(&#10;                        eyeLY * 80,&#10;                        -eyeLX * 80,&#10;                        0.0f&#10;                    )&#10;&#10;                    eyeRight = Triple(&#10;                        eyeRY * 80,&#10;                        eyeRX * 80,&#10;                        0.0f&#10;                    )&#10;&#10;                }&#10;&#10;                if (blendhsapes != null &amp;&amp; blendhsapes.isPresent) {&#10;                    val blendshapesMap = blendhsapes.get()[0].associate {&#10;                        it.categoryName() to it.score()&#10;                    }&#10;                    sendBlendshapesToPC(blendshapesMap, faceRotation, facePosition, eyeLeft, eyeRight)&#10;                } else {&#10;                    Log.d(TAG, &quot;No blendshapes detected&quot;)&#10;                }&#10;&#10;&#10;                if (faceLandmarks != null &amp;&amp; !(faceLandmarks.isEmpty())) {&#10;                    for (noemalizedFaceLandmark in faceLandmarks) {&#10;                        var i = 0&#10;//                        for (landmark in noemalizedFaceLandmark) {&#10;//                            //Log.d(TAG, &quot;Landmark ${landmark.toString()}: ${landmark.x()}, ${landmark.y()}, ${landmark.z()}&quot;)&#10;//                            Log.d(TAG, &quot;Landmark [${i}] : ${landmark.toString()}}&quot;)&#10;//                            i++&#10;//                        }&#10;                        //                       Log.d(&quot;Face LandMarker Landmark0&quot;, &quot;Landmark [0] : ${noemalizedFaceLandmark[0].toString()}&quot;)&#10;                    }&#10;                    //Log.d(&quot;Face LandMarker Landmark0&quot;, &quot;Landmark [0] : ${faceLandmarks[0][0].toString()}&quot;)&#10;&#10;                } else {&#10;                    Log.d(TAG, &quot;No face landmarks detected&quot;)&#10;                }&#10;&#10;&#10;            }&#10;        }&#10;&#10;&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.amoherom.mizuface.fragment&#10;&#10;import android.annotation.SuppressLint&#10;import android.os.Bundle&#10;import android.util.Log&#10;import android.view.LayoutInflater&#10;import android.view.View&#10;import android.view.ViewGroup&#10;import android.widget.Toast&#10;import androidx.camera.core.AspectRatio&#10;import androidx.camera.core.Camera&#10;import androidx.camera.core.CameraSelector&#10;import androidx.camera.core.ImageAnalysis&#10;import androidx.camera.core.ImageProxy&#10;import androidx.camera.core.Preview&#10;import androidx.camera.lifecycle.ProcessCameraProvider&#10;import androidx.core.content.ContextCompat&#10;import androidx.fragment.app.Fragment&#10;import androidx.fragment.app.activityViewModels&#10;import androidx.navigation.Navigation&#10;import androidx.recyclerview.widget.LinearLayoutManager&#10;import androidx.recyclerview.widget.RecyclerView&#10;import com.amoherom.mizuface.FaceLandmarkerHelper&#10;import com.amoherom.mizuface.MainViewModel&#10;import com.amoherom.mizuface.R&#10;import com.amoherom.mizuface.databinding.FragmentVtuberPcBinding&#10;import com.amoherom.mizuface.fragment.FaceBlendshapesResultAdapter&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import kotlinx.coroutines.CoroutineScope&#10;import kotlinx.coroutines.Dispatchers&#10;import kotlinx.coroutines.launch&#10;import java.net.DatagramPacket&#10;import java.net.DatagramSocket&#10;import java.net.InetAddress&#10;import java.nio.charset.Charset&#10;import java.util.concurrent.ExecutorService&#10;import java.util.concurrent.Executors&#10;import androidx.navigation.findNavController&#10;&#10;class VtuberPCFragment : Fragment(), FaceLandmarkerHelper.LandmarkerListener {&#10;&#10;    // This fragment is used to send blendshapes to VSeeFace&#10;    // It can be used to display the blendshapes in a UI or send them over a network&#10;&#10;    private var PC_IP = &quot;192.168.1.2&quot;&#10;    private var PC_PORT = &quot;50509&quot; // VSeeFace default port Vnyan 50509&#10;&#10;&#10;    companion object {&#10;        private const val TAG = &quot;Face Landmarker&quot;&#10;    }&#10;&#10;    private var _binding: FragmentVtuberPcBinding? = null&#10;    private val binding get() = _binding!!&#10;&#10;    private val faceBlendshapesResultAdapter by lazy {&#10;        FaceBlendshapesResultAdapter()&#10;    }&#10;&#10;    private lateinit var faceLandmarkerHelper: FaceLandmarkerHelper&#10;&#10;    private val viewModel: MainViewModel by activityViewModels()&#10;&#10;    private var preview: Preview? = null&#10;    private var imageAnalyzer: ImageAnalysis? = null&#10;    private var camera: Camera? = null&#10;    private var cameraProvider: ProcessCameraProvider? = null&#10;    private var cameraFacing = CameraSelector.LENS_FACING_FRONT&#10;&#10;    private lateinit var backgroundExecutor: ExecutorService&#10;&#10;    private var pcSocket: DatagramSocket? = null&#10;&#10;    override fun onResume() {&#10;        super.onResume()&#10;        // Make sure that all permissions are still present, since the&#10;        // user could have removed them while the app was in paused state.&#10;        if (!PermissionsFragment.hasPermissions(requireContext())) {&#10;            Navigation.findNavController(&#10;                requireActivity(), R.id.fragment_container&#10;            ).navigate(R.id.action_vtuberPCFragment_to_permissions_fragment)&#10;        }&#10;&#10;        // Start the FaceLandmarkerHelper again when users come back&#10;        // to the foreground.&#10;        backgroundExecutor.execute {&#10;            if (faceLandmarkerHelper.isClose()) {&#10;                faceLandmarkerHelper.setupFaceLandmarker()&#10;            }&#10;        }&#10;    }&#10;&#10;    override fun onCreateView(&#10;        inflater: LayoutInflater,&#10;        container: ViewGroup?,&#10;        savedInstanceState: Bundle?&#10;    ): View {&#10;        _binding = FragmentVtuberPcBinding.inflate(inflater, container, false)&#10;        return binding.root&#10;    }&#10;&#10;    override fun onDestroyView() {&#10;        super.onDestroyView()&#10;        _binding = null&#10;        ClosePCConnection()&#10;    }&#10;&#10;    private fun setUpCamera(){&#10;        val cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())&#10;        cameraProviderFuture.addListener(&#10;            {&#10;                cameraProvider = cameraProviderFuture.get()&#10;                bindCameraUseCases()&#10;            }, ContextCompat.getMainExecutor(requireContext())&#10;        )&#10;    }&#10;&#10;    @SuppressLint(&quot;MissingPermission&quot;)&#10;    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {&#10;        super.onViewCreated(view, savedInstanceState)&#10;&#10;        with(binding.recyclerViewResults){&#10;            layoutManager = LinearLayoutManager(requireContext())&#10;            adapter = faceBlendshapesResultAdapter&#10;        }&#10;&#10;        backgroundExecutor = Executors.newSingleThreadExecutor()&#10;&#10;        // Check permissions before setting up camera&#10;        if (!PermissionsFragment.hasPermissions(requireContext())) {&#10;            requireActivity().findNavController(R.id.fragment_container).navigate(R.id.action_vtuberPCFragment_to_permissions_fragment)&#10;            return&#10;        }&#10;&#10;        binding.viewFinder.post{&#10;            setUpCamera()&#10;        }&#10;&#10;        backgroundExecutor.execute {&#10;            faceLandmarkerHelper = FaceLandmarkerHelper(&#10;                context = requireContext(),&#10;                runningMode = RunningMode.LIVE_STREAM,&#10;                minFaceDetectionConfidence = viewModel.currentMinFaceDetectionConfidence,&#10;                minFaceTrackingConfidence = viewModel.currentMinFaceTrackingConfidence,&#10;                minFacePresenceConfidence = viewModel.currentMinFacePresenceConfidence,&#10;                maxNumFaces = viewModel.currentMaxFaces,&#10;                currentDelegate = viewModel.currentDelegate,&#10;                faceLandmarkerHelperListener = this&#10;            )&#10;        }&#10;&#10;        // Initialize the PC connection&#10;        InitiatePCConnection()&#10;    }&#10;&#10;    private fun InitiatePCConnection() {&#10;        // This function can be used to initiate a connection to the PC&#10;        // For example, creating a socket connection or HTTP request&#10;        Log.d(TAG, &quot;Initiating connection to PC at $PC_IP:$PC_PORT&quot;)&#10;        try {&#10;            pcSocket = DatagramSocket()&#10;            binding.pcLinkState.setImageResource(R.drawable.link)&#10;            Log.d(TAG, &quot;PC Connection initiated successfully&quot;)&#10;        } catch (e: Exception) {&#10;            binding.pcLinkState.setImageResource(R.drawable.errorlink)&#10;            Log.e(TAG, &quot;Error initiating PC connection&quot;, e)&#10;        }&#10;    }&#10;&#10;    private fun ClosePCConnection(){&#10;        // This function can be used to close the connection to the PC&#10;        // For example, closing a socket connection or HTTP request&#10;        Log.d(TAG, &quot;Closing connection to PC at $PC_IP:$PC_PORT&quot;)&#10;        // Implement the logic to close the connection here&#10;        try {&#10;            pcSocket?.close()&#10;            Log.d(TAG, &quot;PC Connection closed successfully&quot;)&#10;        } catch (e: Exception) {&#10;            Log.e(TAG, &quot;Error closing PC connection&quot;, e)&#10;        }&#10;    }&#10;    private fun sendBlendshapesToPC(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        Rotation: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        Position: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeRight: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;    ) {&#10;        // This function can be used to send blendshapes to a PC&#10;        // For example, using a socket connection or HTTP request&#10;        // Here we just log the blendshapes&#10;        CoroutineScope(Dispatchers.IO).launch {&#10;            val json = buildJson(blendshapes, Position, Rotation, eyeLeft, eyeRight)&#10;            val buffer = json.toByteArray(Charset.forName(&quot;UTF-8&quot;))&#10;            val packet = DatagramPacket(&#10;                buffer,&#10;                buffer.size,&#10;                InetAddress.getByName(PC_IP),&#10;                PC_PORT.toInt()&#10;            )&#10;&#10;&#10;            if (pcSocket == null) {&#10;                binding.pcLinkState.setImageResource(R.drawable.errorlink)&#10;                Log.e(TAG, &quot;PC Socket is not initialized. Cannot send blendshapes.&quot;)&#10;            }&#10;&#10;            pcSocket?.send(packet)&#10;            //Log.d(TAG, &quot;Blendshapes sent to PC: $json&quot;)&#10;        }&#10;&#10;    }&#10;&#10;    private fun buildJson(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        position: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        rotation: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f),&#10;        eyeRight: Triple&lt;Float, Float, Float&gt; = Triple(0.0f, 0.0f, 0.0f)&#10;    ): String {&#10;&#10;        val finalJson = buildTrackerFaceJson(&#10;            blendshapes = blendshapes,&#10;            position = position, // Placeholder for position&#10;            rotation = rotation, // Placeholder for rotation&#10;            vnYanPos = Triple(0.0f, 0.0f, 0.0f), // Placeholder for VNyan position&#10;            eyeLeft = eyeLeft, // Placeholder for left eye position&#10;            eyeRight = eyeRight // Placeholder for right eye position&#10;        )&#10;&#10;        return finalJson&#10;    }&#10;&#10;    private fun buildTrackerFaceJson(&#10;        blendshapes: Map&lt;String, Float&gt;,&#10;        rotation: Triple&lt;Float, Float, Float&gt;,&#10;        position: Triple&lt;Float, Float, Float&gt;,&#10;        vnYanPos: Triple&lt;Float, Float, Float&gt;,&#10;        eyeLeft: Triple&lt;Float, Float, Float&gt;,&#10;        eyeRight: Triple&lt;Float, Float, Float&gt;&#10;    ): String {&#10;        val blendshapesJson = blendshapes.entries.joinToString(&quot;,&quot;) {&#10;            &quot;&quot;&quot;{&quot;k&quot;:&quot;${it.key}&quot;,&quot;v&quot;:${it.value}}&quot;&quot;&quot;&#10;        }&#10;&#10;        return &quot;&quot;&quot;&#10;    {&#10;        &quot;Timestamp&quot;: ${System.currentTimeMillis()},&#10;        &quot;Hotkey&quot;: -1,&#10;        &quot;FaceFound&quot;: true,&#10;        &quot;Rotation&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, rotation.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, rotation.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, rotation.third)} },&#10;        &quot;Position&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, position.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, position.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, position.third)} },&#10;        &quot;EyeLeft&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, eyeLeft.third)} },&#10;        &quot;EyeRight&quot;: { &quot;x&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.first)}, &quot;y&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.second)}, &quot;z&quot;: ${String.format(&quot;%.8f&quot;, eyeRight.third)} },&#10;        &quot;BlendShapes&quot;: [ $blendshapesJson ]&#10;    }&#10;    &quot;&quot;&quot;.trimIndent().replace(Regex(&quot;\\s+&quot;), &quot;&quot;)&#10;    }&#10;&#10;&#10;    @SuppressLint(&quot;UnsafeOptInUsageError&quot;)&#10;    private fun bindCameraUseCases() {&#10;&#10;        // CameraProvider&#10;        val cameraProvider = cameraProvider&#10;            ?: throw IllegalStateException(&quot;Camera initialization failed.&quot;)&#10;&#10;        val cameraSelector =&#10;            CameraSelector.Builder().requireLensFacing(cameraFacing).build()&#10;&#10;        // Preview. Only using the 4:3 ratio because this is the closest to our models&#10;        preview = Preview.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)&#10;            .setTargetRotation(binding.viewFinder.display.rotation)&#10;            .build()&#10;&#10;        // ImageAnalysis. Using RGBA 8888 to match how our models work&#10;        imageAnalyzer =&#10;            ImageAnalysis.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)&#10;                .setTargetRotation(binding.viewFinder.display.rotation)&#10;                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)&#10;                .setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)&#10;                .build()&#10;                // The analyzer can then be assigned to the instance&#10;                .also {&#10;                    it.setAnalyzer(backgroundExecutor) { image -&gt;&#10;                        detectFace(image)&#10;                    }&#10;                }&#10;&#10;        // Must unbind the use-cases before rebinding them&#10;        cameraProvider.unbindAll()&#10;&#10;        try {&#10;            // A variable number of use-cases can be passed here -&#10;            // camera provides access to CameraControl &amp; CameraInfo&#10;            camera = cameraProvider.bindToLifecycle(&#10;                this, cameraSelector, preview, imageAnalyzer&#10;            )&#10;&#10;            // Attach the viewfinder's surface provider to preview use case&#10;            preview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)&#10;        } catch (exc: Exception) {&#10;            Log.e(TAG, &quot;Use case binding failed&quot;, exc)&#10;        }&#10;    }&#10;&#10;    private fun detectFace(imageProxy: ImageProxy) {&#10;        faceLandmarkerHelper.detectLiveStream(&#10;            imageProxy = imageProxy,&#10;            isFrontCamera = cameraFacing == CameraSelector.LENS_FACING_FRONT&#10;        )&#10;    }&#10;&#10;    override fun onError(error: String, errorCode: Int) {&#10;        activity?.runOnUiThread {&#10;            Toast.makeText(requireContext(), error, Toast.LENGTH_SHORT).show()&#10;            faceBlendshapesResultAdapter.updateResults(null)&#10;            faceBlendshapesResultAdapter.notifyDataSetChanged()&#10;&#10;            if (errorCode == FaceLandmarkerHelper.GPU_ERROR) {&#10;                Log.d(&quot;VtuberPCFragment&quot;, &quot;GPU_ERROR&quot; + errorCode.toString())&#10;            }&#10;        }&#10;    }&#10;&#10;    fun toAvatarEyeCoords(x: Float, y: Float): Triple&lt;Float, Float, Float&gt; {&#10;        val scaledX = (x - 0.5f) * 40f // center and scale&#10;        val scaledY = (0.5f - y) * 50f // flip Y and scale&#10;        return Triple(scaledX, scaledY, 0f)&#10;    }&#10;&#10;    fun calcEyeOffsetXY(iris: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        left: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        right: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        top: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?,&#10;                        bottom: com.google.mediapipe.tasks.components.containers.NormalizedLandmark?&#10;    ): Pair&lt;Float, Float&gt; {&#10;        val horizontal = (((iris?.x() ?: 0f) - left?.x()!!) / ((right?.x() ?: 0f) - left.x()) - 0.5f) * 2f // -1 to 1&#10;        val vertical = (((iris?.y() ?: 0f ) - top?.y()!!) / ((bottom?.y() ?: 0f) - top.y()) - 0.5f) * 2f&#10;        return Pair(horizontal, vertical)&#10;    }&#10;&#10;&#10;&#10;    override fun onResults(resultBundle: FaceLandmarkerHelper.ResultBundle) {&#10;        activity?.runOnUiThread {&#10;            if (_binding != null){&#10;                if(binding.recyclerViewResults.scrollState != RecyclerView.SCROLL_STATE_DRAGGING) {&#10;                    faceBlendshapesResultAdapter.updateResults(resultBundle.result)&#10;                    faceBlendshapesResultAdapter.notifyDataSetChanged()&#10;                }&#10;&#10;                var blendhsapes = resultBundle.result?.faceBlendshapes()&#10;                var faceLandmarks = resultBundle.result?.faceLandmarks()&#10;                var noseLandmarkindex = faceLandmarks?.get(0)?.get(1)&#10;                var mouthLandmarkIndex = faceLandmarks?.get(0)?.get(0)&#10;&#10;                var leftEyeLandmarkIndex = faceLandmarks?.get(0)?.get(33)&#10;                var leftEyeLEFT = faceLandmarks?.get(0)?.get(133)&#10;                var leftEyeRIGHT = faceLandmarks?.get(0)?.get(33)&#10;                var leftEyeTOP = faceLandmarks?.get(0)?.get(159)&#10;                var leftEyeBOTTOM = faceLandmarks?.get(0)?.get(145)&#10;&#10;                var leftIrisLandmarkIndex = faceLandmarks?.get(0)?.get(468)&#10;                var leftIrisx = leftIrisLandmarkIndex?.x()?.toFloat() ?: 0f&#10;                var leftIrisy = leftIrisLandmarkIndex?.y()?.toFloat() ?: 0f&#10;&#10;                var rightEyeLandmarkIndex = faceLandmarks?.get(0)?.get(263)&#10;                var rightEyeLEFT = faceLandmarks?.get(0)?.get(362)&#10;                var rightEyeRIGHT = faceLandmarks?.get(0)?.get(263)&#10;                var rightEyeTOP = faceLandmarks?.get(0)?.get(386)&#10;                var rightEyeBOTTOM = faceLandmarks?.get(0)?.get(374)&#10;&#10;                var rightIrisLandmarkIndex = faceLandmarks?.get(0)?.get(473)&#10;                var rightIrisx = rightIrisLandmarkIndex?.x()?.toFloat() ?: 0f&#10;                var rightIrisy = rightIrisLandmarkIndex?.y()?.toFloat() ?: 0f&#10;&#10;                val (eyeLX, eyeLY) = calcEyeOffsetXY(&#10;                    iris = leftIrisLandmarkIndex,&#10;                    left = leftEyeLEFT,&#10;                    right = leftEyeRIGHT,&#10;                    top = leftEyeTOP,&#10;                    bottom = leftEyeBOTTOM&#10;                )&#10;&#10;                val (eyeRX, eyeRY) = calcEyeOffsetXY(&#10;                    iris = rightIrisLandmarkIndex,&#10;                    left = rightEyeLEFT,&#10;                    right = rightEyeRIGHT,&#10;                    top = rightEyeTOP,&#10;                    bottom = rightEyeBOTTOM&#10;                )&#10;&#10;                //Log.d(&quot;EyeTrack&quot;, &quot;LeftEye X: $eyeLX, Y: $eyeLY&quot;)&#10;&#10;                // upto 468 landmarks is the base&#10;                // with iris its up to 478 landmarks&#10;&#10;                // Yaw (side turn) → difference in eye X positions&#10;                val dxYaw = (rightEyeLandmarkIndex?.x() ?: 0f) - (leftEyeLandmarkIndex?.x() ?: 0f)&#10;                val dyYaw = (rightEyeLandmarkIndex?.z() ?: 0f) - (leftEyeLandmarkIndex?.z() ?: 0f)&#10;                val yaw = Math.atan2(dxYaw.toDouble(), dyYaw.toDouble()).toFloat()&#10;&#10;// Pitch (up/down) → difference between nose and eyes&#10;                val eyeY = ((leftEyeLandmarkIndex?.y() ?: 0f) + (rightEyeLandmarkIndex?.y() ?: 0f)) / 2&#10;                val pitch = Math.atan2((noseLandmarkindex?.y() ?: 0f) - eyeY.toDouble(), 1.0).toFloat()&#10;&#10;// Roll stays same (tilt head side)&#10;                val roll = Math.atan2(&#10;                    ((rightEyeLandmarkIndex?.y() ?: 0f) - (leftEyeLandmarkIndex?.y() ?: 0f)).toDouble(),&#10;                    ((rightEyeLandmarkIndex?.x() ?: 0f) - (leftEyeLandmarkIndex?.x() ?: 0f)).toDouble()&#10;                ).toFloat()&#10;&#10;&#10;&#10;//                var faceRotation = Triple((pitch * 100 / Math.PI).toFloat(), (yaw ).toFloat(), (roll * 100  / Math.PI).toFloat())&#10;                var faceRotation = Triple(&#10;                    50f - (yaw.toFloat() * 90 / Math.PI.toFloat()),&#10;                    pitch.toFloat() * 1000f / Math.PI.toFloat(),&#10;                    -roll.toFloat() * 100f / Math.PI.toFloat(),&#10;                )&#10;                var facePosition = Triple(0.0f, 0.0f, 0.0f)&#10;                var eyeLeft = Triple(0.5f, 0.5f, 0.5f)&#10;                var eyeRight = Triple(0.5f, 0.5f, 0.5f)&#10;&#10;                if (faceLandmarks != null &amp;&amp; faceLandmarks.isNotEmpty()) {&#10;                    facePosition = Triple(&#10;                        0f,&#10;                        0f,&#10;                        0f&#10;                    )&#10;&#10;                    eyeLeft = Triple(&#10;                        eyeLY * 80,&#10;                        -eyeLX * 80,&#10;                        0.0f&#10;                    )&#10;&#10;                    eyeRight = Triple(&#10;                        eyeRY * 80,&#10;                        eyeRX * 80,&#10;                        0.0f&#10;                    )&#10;&#10;                }&#10;&#10;                if (blendhsapes != null &amp;&amp; blendhsapes.isPresent) {&#10;                    val blendshapesMap = blendhsapes.get()[0].associate {&#10;                        it.categoryName() to it.score()&#10;                    }&#10;                    sendBlendshapesToPC(blendshapesMap, faceRotation, facePosition, eyeLeft, eyeRight)&#10;                } else {&#10;                    Log.d(TAG, &quot;No blendshapes detected&quot;)&#10;                }&#10;&#10;&#10;                if (faceLandmarks != null &amp;&amp; !(faceLandmarks.isEmpty())) {&#10;                    for (noemalizedFaceLandmark in faceLandmarks) {&#10;                        var i = 0&#10;//                        for (landmark in noemalizedFaceLandmark) {&#10;//                            //Log.d(TAG, &quot;Landmark ${landmark.toString()}: ${landmark.x()}, ${landmark.y()}, ${landmark.z()}&quot;)&#10;//                            Log.d(TAG, &quot;Landmark [${i}] : ${landmark.toString()}}&quot;)&#10;//                            i++&#10;//                        }&#10;                        //                       Log.d(&quot;Face LandMarker Landmark0&quot;, &quot;Landmark [0] : ${noemalizedFaceLandmark[0].toString()}&quot;)&#10;                    }&#10;                    //Log.d(&quot;Face LandMarker Landmark0&quot;, &quot;Landmark [0] : ${faceLandmarks[0][0].toString()}&quot;)&#10;&#10;                } else {&#10;                    Log.d(TAG, &quot;No face landmarks detected&quot;)&#10;                }&#10;&#10;&#10;            }&#10;        }&#10;&#10;&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/src/main/res/layout/activity_main.xml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/res/layout/activity_main.xml" />
              <option name="originalContent" value="&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;&#10;             android:layout_width=&quot;match_parent&quot;&#10;             android:layout_height=&quot;match_parent&quot;&gt;&#10;" />
              <option name="updatedContent" value="&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;&#10;    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;&#10;    android:layout_width=&quot;match_parent&quot;&#10;    android:layout_height=&quot;match_parent&quot;&gt;&#10;&#10;    &lt;androidx.fragment.app.FragmentContainerView&#10;        android:id=&quot;@+id/fragment_container&quot;&#10;        android:name=&quot;androidx.navigation.fragment.NavHostFragment&quot;&#10;        android:layout_width=&quot;0dp&quot;&#10;        android:layout_height=&quot;0dp&quot;&#10;        app:layout_constraintBottom_toBottomOf=&quot;parent&quot;&#10;        app:layout_constraintEnd_toEndOf=&quot;parent&quot;&#10;        app:layout_constraintStart_toStartOf=&quot;parent&quot;&#10;        app:layout_constraintTop_toTopOf=&quot;parent&quot;&#10;        app:defaultNavHost=&quot;true&quot;&#10;        app:navGraph=&quot;@navigation/nav_graph&quot; /&gt;&#10;&#10;&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/src/main/res/layout/fragment_permissions.xml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/res/layout/fragment_permissions.xml" />
              <option name="updatedContent" value="&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;&#10;    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;&#10;    xmlns:tools=&quot;http://schemas.android.com/tools&quot;&#10;    android:layout_width=&quot;match_parent&quot;&#10;    android:layout_height=&quot;match_parent&quot;&#10;    android:padding=&quot;24dp&quot;&gt;&#10;&#10;    &lt;TextView&#10;        android:id=&quot;@+id/text_permission_title&quot;&#10;        android:layout_width=&quot;wrap_content&quot;&#10;        android:layout_height=&quot;wrap_content&quot;&#10;        android:text=&quot;Camera Permission Required&quot;&#10;        android:textSize=&quot;24sp&quot;&#10;        android:textStyle=&quot;bold&quot;&#10;        app:layout_constraintBottom_toTopOf=&quot;@+id/text_permission_explanation&quot;&#10;        app:layout_constraintEnd_toEndOf=&quot;parent&quot;&#10;        app:layout_constraintStart_toStartOf=&quot;parent&quot;&#10;        app:layout_constraintTop_toTopOf=&quot;parent&quot;&#10;        app:layout_constraintVertical_chainStyle=&quot;packed&quot; /&gt;&#10;&#10;    &lt;TextView&#10;        android:id=&quot;@+id/text_permission_explanation&quot;&#10;        android:layout_width=&quot;match_parent&quot;&#10;        android:layout_height=&quot;wrap_content&quot;&#10;        android:layout_marginTop=&quot;16dp&quot;&#10;        android:text=&quot;This app requires camera permission to track your facial expressions and movements for the VTuber functionality. Without camera access, the app cannot work.&quot;&#10;        android:textAlignment=&quot;center&quot;&#10;        android:textSize=&quot;16sp&quot;&#10;        android:visibility=&quot;visible&quot;&#10;        app:layout_constraintBottom_toTopOf=&quot;@+id/button_request_permission&quot;&#10;        app:layout_constraintEnd_toEndOf=&quot;parent&quot;&#10;        app:layout_constraintStart_toStartOf=&quot;parent&quot;&#10;        app:layout_constraintTop_toBottomOf=&quot;@+id/text_permission_title&quot; /&gt;&#10;&#10;    &lt;Button&#10;        android:id=&quot;@+id/button_request_permission&quot;&#10;        android:layout_width=&quot;wrap_content&quot;&#10;        android:layout_height=&quot;wrap_content&quot;&#10;        android:layout_marginTop=&quot;24dp&quot;&#10;        android:text=&quot;Grant Camera Permission&quot;&#10;        app:layout_constraintBottom_toBottomOf=&quot;parent&quot;&#10;        app:layout_constraintEnd_toEndOf=&quot;parent&quot;&#10;        app:layout_constraintStart_toStartOf=&quot;parent&quot;&#10;        app:layout_constraintTop_toBottomOf=&quot;@+id/text_permission_explanation&quot; /&gt;&#10;&#10;&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>